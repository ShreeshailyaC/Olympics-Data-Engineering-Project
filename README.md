# Olympics-Data-Engineering-Project

An end-to-end modern data engineering pipeline built using the Olympics dataset from Kaggle. This project demonstrates how to ingest, transform, orchestrate, and govern data at scale using Azure and Databricks.

This project walks through the full lifecycle of a production-style data pipeline:

Ingestion: Pulled data from GitHub into Azure Data Lake using Azure Data Factory with Git integration and parameterized pipelines.

Processing & Transformations: Used Databricks, PySpark, Unity Catalog, and Delta Live Tables to clean, transform, and model large datasets.

Advanced Engineering Features: Implemented Spark Structured Streaming, Change Data Capture (CDC), and scalable processing on Apache Spark clusters.

Orchestration & Automation: Managed workflows with Databricks Workflows and automated version control + CI/CD using GitHub.

Tech Stack :

Azure Data Factory | Azure Data Lake | Databricks | PySpark | Delta Live Tables | Unity Catalog | Spark Structured Streaming | CDC | GitHub (CI/CD)


Dataset Link : https://www.kaggle.com/datasets/piterfm/paris-2024-olympic-summer-games
